# agent.py

import config
import vector_store_interface
import llm_interface
import search_tools
from typing import Optional
import traceback # Import for better error logging if needed later

class AgenticRAG:
    """
    The main Agentic RAG class orchestrating the process.
    Includes option to disable web search fallback.
    """
    def __init__(self):
        """Initializes the agent components."""
        print("Initializing Agentic RAG...")
        try:
            self.vector_store = vector_store_interface.QdrantVectorStore()
        except ConnectionError as e:
             print(f"FATAL ERROR during Agent initialization: Could not connect to Vector Store at {config.QDRANT_URL}.")
             print(f"  Ensure Qdrant Docker container is running and accessible.")
             print(f"  Error details: {e}")
             self.vector_store = None
        except Exception as e:
            print(f"FATAL ERROR: An unexpected error occurred during Agent initialization:")
            traceback.print_exc()
            self.vector_store = None

    def process_query(self, question: str) -> str:
        """
        Processes a user query through the Agentic RAG pipeline.

        Args:
            question (str): The user's question.

        Returns:
            str: The final answer generated by the LLM or a status message.
        """
        # Define consistent response messages
        answer_error = "Sorry, I encountered an error and couldn't generate an answer."
        answer_no_local_info_web_disabled = "Based on the ingested documents, I cannot answer this question, and web search fallback is disabled."
        answer_no_local_info_web_failed = "Based on the ingested documents, I cannot answer this question. The subsequent web search also failed."

        # Check if vector store is available
        if self.vector_store is None:
             print("Error: Vector store is not available. Cannot process query.")
             return answer_error

        print(f"\nProcessing query: '{question}'")

        # 1. Retrieve context from Vector Store
        print("\nStep 1: Retrieving context from Vector Store...")
        try:
            retrieved_docs = self.vector_store.search(question, top_k=config.RETRIEVAL_TOP_K)
            retrieved_context = self.vector_store.format_search_results(retrieved_docs)
            # You can still keep this debugging print if you like:
            # print("\n--- Context Retrieved from Vector Store (for LLM Decision) ---")
            # print(retrieved_context)
            # print("--- End of Retrieved Context ---")
        except Exception as e:
            print(f"ERROR: Failed to retrieve context from vector store: {e}")
            traceback.print_exc()
            return answer_error # Return error if search fails

        # 2. Decide if retrieved context is sufficient
        print("\nStep 2: Asking LLM to decide if context is sufficient...")
        decision = llm_interface.get_llm_decision(retrieved_context, question)

        final_answer = answer_error # Default answer if subsequent steps fail
        context_to_use = ""
        source_type = "None" # Start with no source

        # 3. Act based on decision
        if decision == '1':
            print("Decision: Context is relevant. Generating answer from retrieved context...")
            context_to_use = retrieved_context
            source_type = "Vector Store"
            # Proceed directly to step 5 (answer generation)

        elif decision == '0':
            print("Decision: Context is NOT relevant.")
            # <<< --- Check if web search fallback is allowed --- >>>
            if config.ALLOW_WEB_SEARCH_FALLBACK:
                print("Web search fallback ENABLED. Searching online...")
                # 4. Fallback: Search the web
                web_results = search_tools.web_search(question)

                if web_results is None: # Check if web search itself failed
                     print("ERROR: Web search failed. Cannot proceed with web context.")
                     # Use the specific error message for this case
                     return answer_no_local_info_web_failed

                web_context = search_tools.format_search_results(web_results)
                # print(f"\n--- Context from Web Search (for Final Answer) ---")
                # print(web_context)
                # print("--- End of Web Context ---")
                context_to_use = web_context
                source_type = "Web Search"
                # Proceed to step 5 (answer generation) using web context

            else:
                # <<< --- Web search fallback is DISABLED --- >>>
                print("Web search fallback DISABLED. Cannot answer from ingested documents.")
                # No context to use, return specific message
                return answer_no_local_info_web_disabled

        else: # Handle None or unexpected value from decision LLM
             print(f"Error: Could not get a valid decision ('0' or '1') from LLM. Received: '{decision}'. Cannot proceed.")
             return answer_error

        # 5. Generate Final Answer (only if context_to_use is available)
        print(f"\nStep 3: Generating final answer using context from {source_type}...")
        if not context_to_use:
             # This case should theoretically be handled by the decision logic above,
             # but added as a safeguard.
             print("Internal Error: No context was prepared for final answer generation.")
             return answer_error

        final_answer_llm = llm_interface.get_llm_answer(context_to_use, question)

        if final_answer_llm and final_answer_llm != answer_error: # Check success
            final_answer = final_answer_llm
            print("\nFinal Answer Generation Complete.")
        else:
            # get_llm_answer prints its own errors, message is already set to error/failure
            print("Warning/Error: Final answer generation using LLM failed or returned an error message.")
            # Keep the message from get_llm_answer or the default error

        return final_answer